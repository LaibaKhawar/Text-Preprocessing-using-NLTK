{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60023408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "046d8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"bbc_articles.csv\")\n",
    "df['text'] = df['title'] + \". \" + df['brief']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd036fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       uk considering recognising palestine state, lo...\n",
       "1       ny police probe alleged attack on propalestine...\n",
       "2       propalestine group targets twickenham stadium ...\n",
       "3       palestine action says it vandalised haulage fi...\n",
       "4       israelgaza london school warns it may close ov...\n",
       "                              ...                        \n",
       "1492    ks3  ks4  gcse history un partition plan for p...\n",
       "1493    two men admit verbally abusing propalestine pr...\n",
       "1494    palestine action arms factory rooftop proteste...\n",
       "1495    birmingham colmore row building targeted by pr...\n",
       "1496    tories suspend oldham councillors who went on ...\n",
       "Name: cleaned_text, Length: 1497, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Noise Removal\n",
    "def remove_noise(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    # Remove special characters except periods and commas\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s,.]', '', text)\n",
    "    # Convert to lowercase\n",
    "    return text.lower()\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(remove_noise)\n",
    "df['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22dbd92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       uk considering recognising palestine state , l...\n",
      "1       ny police probe alleged attack propalestine ma...\n",
      "2       propalestine group target twickenham stadium r...\n",
      "3       palestine action say vandalised haulage firm ....\n",
      "4       israelgaza london school warns may close pales...\n",
      "                              ...                        \n",
      "1492    ks3 ks4 gcse history un partition plan palesti...\n",
      "1493    two men admit verbally abusing propalestine pr...\n",
      "1494    palestine action arm factory rooftop protester...\n",
      "1495    birmingham colmore row building targeted propa...\n",
      "1496    tory suspend oldham councillor went propalesti...\n",
      "Name: normalized_text_lemma, Length: 1497, dtype: object \n",
      "\n",
      "\n",
      " 0       uk consid recognis palestin state , lord camer...\n",
      "1       ny polic probe alleg attack propalestin march ...\n",
      "2       propalestin group target twickenham stadium re...\n",
      "3       palestin action say vandalis haulag firm . red...\n",
      "4       israelgaza london school warn may close palest...\n",
      "                              ...                        \n",
      "1492    ks3 ks4 gcse histori un partit plan palestin e...\n",
      "1493    two men admit verbal abus propalestin protest ...\n",
      "1494    palestin action arm factori rooftop protest se...\n",
      "1495    birmingham colmor row build target propalestin...\n",
      "1496    tori suspend oldham councillor went propalesti...\n",
      "Name: normalized_text_stem, Length: 1497, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stemming and Lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stem_lemma_stop(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed = [stemmer.stem(token) for token in tokens]\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stop_words_set = set(stopwords.words('english'))\n",
    "    no_stop_words_lemma = [word for word in lemmatized if word not in stop_words_set]\n",
    "    no_stop_words_stem = [word for word in stemmed if word not in stop_words_set]\n",
    "    return (' '.join(no_stop_words_lemma),' '.join(no_stop_words_stem))\n",
    "\n",
    "df[['normalized_text_lemma', 'normalized_text_stem']] = df['cleaned_text'].apply(stem_lemma_stop).apply(pd.Series)\n",
    "print(df['normalized_text_lemma'],\"\\n\\n\\n\",df['normalized_text_stem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "523c6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll be going with lemmatized text instead of stemmed text as lemmatization clumps similar text to a single value\n",
    "\n",
    "# N-grams\n",
    "def extract_ngrams(text, num):\n",
    "    n_grams = ngrams(word_tokenize(text), num)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# Combine all texts to a single corpus for n-gram analysis\n",
    "corpus = ' '.join(df['normalized_text_lemma'].tolist())\n",
    "# Extracting N-grams\n",
    "unigrams = extract_ngrams(corpus, 1)\n",
    "bigrams = extract_ngrams(corpus, 2)\n",
    "trigrams = extract_ngrams(corpus, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c54ba77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Unigrams: [('.', 3063), ('palestine', 1245), (',', 1110), ('propalestine', 627), ('say', 526), ('action', 494), ('state', 397), ('protester', 379), ('london', 378), ('israel', 352)] \n",
      "\n",
      "Top 10 Bigrams: [('. palestine', 396), ('palestine action', 370), ('israel .', 257), ('palestine state', 248), ('say .', 247), ('propalestine march', 245), ('red paint', 245), ('partition plan', 245), ('march .', 244), ('propalestine group', 244)] \n",
      "\n",
      "Top 10 Trigrams: [('. palestine action', 368), ('propalestine march .', 244), ('un partition plan', 244), ('partition plan palestine', 244), ('recognise palestine state', 125), ('recognising palestine state', 123), ('red paint .', 123), ('. ks3 ks4', 123), ('ks3 ks4 gcse', 123), ('ks4 gcse history', 123)]\n"
     ]
    }
   ],
   "source": [
    "# Frequency analysis\n",
    "def get_top_ngrams(ngram_list, n=10):\n",
    "    freq_dist = nltk.FreqDist(ngram_list)\n",
    "    return freq_dist.most_common(n)\n",
    "\n",
    "print(\"Top 10 Unigrams:\", get_top_ngrams(unigrams),\"\\n\")\n",
    "print(\"Top 10 Bigrams:\", get_top_ngrams(bigrams),\"\\n\")\n",
    "print(\"Top 10 Trigrams:\", get_top_ngrams(trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6d22b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [uk, considering, recognising, palestine, stat...\n",
      "1       [ny, police, probe, alleged, attack, propalest...\n",
      "2       [propalestine, group, target, twickenham, stad...\n",
      "3       [palestine, action, say, vandalised, haulage, ...\n",
      "4       [israelgaza, london, school, warns, may, close...\n",
      "                              ...                        \n",
      "1492    [ks3, ks4, gcse, history, un, partition, plan,...\n",
      "1493    [two, men, admit, verbally, abusing, propalest...\n",
      "1494    [palestine, action, arm, factory, rooftop, pro...\n",
      "1495    [birmingham, colmore, row, building, targeted,...\n",
      "1496    [tory, suspend, oldham, councillor, went, prop...\n",
      "Name: word_tokens, Length: 1497, dtype: object \n",
      "\n",
      "\n",
      " 0       [UK considering recognising Palestine state, L...\n",
      "1       [NY police probe alleged attack on pro-Palesti...\n",
      "2       [Pro-Palestine group targets Twickenham Stadiu...\n",
      "3       [Palestine Action says it vandalised haulage f...\n",
      "4       [Israel-Gaza: London school warns it may close...\n",
      "                              ...                        \n",
      "1492    [KS3 / KS4 / GCSE History: UN partition plan f...\n",
      "1493    [Two men admit verbally abusing pro-Palestine ...\n",
      "1494    [Palestine Action arms factory rooftop protest...\n",
      "1495    [Birmingham: Colmore Row building targeted by ...\n",
      "1496    [Tories suspend Oldham councillors who went on...\n",
      "Name: sentence_tokens, Length: 1497, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Word-level Tokenization\n",
    "df['word_tokens'] = df['normalized_text'].apply(word_tokenize)\n",
    "# Sentence-level Tokenization\n",
    "df['sentence_tokens'] = df['text'].apply(sent_tokenize)\n",
    "\n",
    "print(df['word_tokens'],\"\\n\\n\\n\",df['sentence_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e6ea52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [uk, considering, rec, ##og, ##nis, ##ing, pal...\n",
       "1       [ny, police, probe, alleged, attack, on, pro, ...\n",
       "2       [pro, -, palestine, group, targets, t, ##wick,...\n",
       "3       [palestine, action, says, it, van, ##dal, ##is...\n",
       "4       [israel, -, gaza, :, london, school, warns, it...\n",
       "                              ...                        \n",
       "1492    [ks, ##3, /, ks, ##4, /, g, ##cs, ##e, history...\n",
       "1493    [two, men, admit, verbal, ##ly, abu, ##sing, p...\n",
       "1494    [palestine, action, arms, factory, rooftop, pr...\n",
       "1495    [birmingham, :, col, ##more, row, building, ta...\n",
       "1496    [tori, ##es, suspend, oldham, councillors, who...\n",
       "Name: subword_tokens_bert, Length: 1497, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modern NLP techniques (BERT etc)\n",
    "\n",
    "# Subword Tokenization\n",
    "\n",
    "# tokenize a piece of text using BERT tokenizer\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# BERT tokenize the text\n",
    "df['subword_tokens_bert'] = df['text'].apply(tokenize_text)\n",
    "df['subword_tokens_bert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d5ff798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0af938a00884e2bb7209488466063ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       [[[-0.85742253, 0.18792677, -0.24154675, -0.40...\n",
       "1       [[[-0.5532259, -0.15368998, -0.14605772, -0.36...\n",
       "2       [[[-1.0767837, -0.076922745, -0.015406564, -0....\n",
       "3       [[[-0.58144146, -0.2473306, -0.3095854, -0.676...\n",
       "4       [[[-0.50207776, -0.34874398, -0.083566695, -0....\n",
       "                              ...                        \n",
       "1492    [[[-0.4664716, -0.063210584, -0.670615, -0.185...\n",
       "1493    [[[-0.82238173, -0.20111781, -0.26917163, -1.0...\n",
       "1494    [[[-0.5953143, -0.24393675, 0.16158737, -0.611...\n",
       "1495    [[[-0.5854618, -0.2701547, -0.20922962, -0.624...\n",
       "1496    [[[-0.8234787, 0.06940951, 0.10707852, -0.6584...\n",
       "Name: contextualized_embeddings, Length: 1497, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contextualized Tokenization\n",
    "\n",
    "#basically generating vector embeddings for the text corpus\n",
    "\n",
    "# BERT model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# encode text and get contextualized embeddings\n",
    "def get_contextualized_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Get embeddings\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    # Convert embeddings to np array\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "# Get embeddings\n",
    "df['contextualized_embeddings'] = df['text'].apply(get_contextualized_embeddings)\n",
    "df['contextualized_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3e562ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tokenization generates word embeddings in a vector format, keeping the context of the whole corpus in view.\n",
    "# This preprocessing step is useful for tasks such as feeding this data into ML/DL models for performing downstream\n",
    "# tasks such as text classification, sentiment classification, clustering, or anything related to text/NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
